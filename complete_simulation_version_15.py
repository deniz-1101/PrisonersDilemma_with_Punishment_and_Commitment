# -*- coding: utf-8 -*-
"""Thesis - Complete Simulation - version 15

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBhFV8TB6Zkyf_MG9U-oZaKKpTcUn-kn


version = "15.10"

#libraries used
import numpy as np
import matplotlib.pyplot as plt
import datetime
np.set_printoptions(suppress=True, formatter={'float': '{:0.4f}'.format})

#Population parameters
s = 50 #nr of players
w = 0.1 #intensity of selection
u = 0.01 #mutation probability
#p = 0.1 #observation probability
t_max = (2**3) * (10**5)
t_timesteps = (10**4)
t_snapshots = int(t_max/t_timesteps)

#Game payoff structure, assumptions to hold: c3>c1, c2 is low, b is high
c1 = 2 #cost of coop. when the other defects
c2 = 1 #cost of punishment to the punisher
c3 = 3 #cost of punishmet to the punishee
b = 4 #benefit of defection when the other cooperates

epsilon = 0.01 #stopping rule
np.random.seed(40) #setting the seed

#prep for the observability probability analysis
prob_analysis = np.zeros((11,3)) #first column is the values of p, second is population avg. cooperation rate per given p, third is the population avg. punishment rate per given p
#prep for the parameter analysis
c_snapshots1 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of cooperation when P/NP is not observed, at every 10K periods
c_snapshots2 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of cooperation when P/NP is not observed, at every 10K periods
c_snapshots3 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of cooperation when P/NP is not observed, at every 10K periods
p_snapshots1 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of commitment to punishing, at every 10K periods
p_snapshots2 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of commitment to punishing, at every 10K periods
p_snapshots3 = np.zeros((t_snapshots+1, 1)) #matrix to record avg. population rate of commitment to punishing, at every 10K periods

start_time = datetime.datetime.now()
timestamp_str = start_time.strftime("%Y-%m-%d %H:%M:%S")
print("start time is:", timestamp_str)

#now we do the p analysis for cooperation and punishment averages
for p in np.arange(0, 1.1, 0.1):
  #initializing the population and the matrices for each given p iteration, this is before the game starts
  avg_punishment_rate_over_time = np.zeros((t_max,1))
  avg_cooperation_rate_over_time_not_observed = np.zeros((t_max,1))
  m_final_punishment_rate = np.zeros((s,1))
  m_final_cooperation_rate_not_observed = np.zeros((s,1))

  counter1 = 0 #number of times mutation occurred at reproduction
  counter2 = 0 #number of times mutation occurred due to fixation
  stop = 0

  m = np.random.randint(low=0, high=2, size=(s,2)) #2(pure) strategies are: C/D when unobserved and P/NP. Observed strats are constant: C when observes P, D when observes NP
  m_new = np.zeros((s,2))
  m_total = m

  avg_cooperation_rate_over_time_not_observed[0,0] = sum(m[:,0])/s
  avg_punishment_rate_over_time[0,0] = sum(m[:, 1])/s

  #playing the game now with the given p
  t = 2
  while t <= t_max:
    print("p is:", p, " and t is:", t)
    payoff = np.zeros((s,1)) #the initial empty payoff vector
    probs = np.random.random((s,1)) #P/NP observation probabilities for each of the s agents
    for i in np.arange(s):
      for j in np.arange(s):
        if j != i:
          if probs[i, 0] > p and probs[j, 0] > p: #neither player can observe others' commitment
              if m[i, 0] == 1 and m[j, 0] == 1 : #both cooperate
                    payoff[i, 0] += w * (b - c1)
                    payoff[j, 0] += w * (b - c1)
              elif m[i, 0] == 1 and m[i, 1] == m[j, 0] == 0: #i cooperates j defects, cooperator does not punish
                    payoff[i, 0] += w * (-c1)
                    payoff[j, 0] += w * b
              elif m[i, 0] == m[i, 1] == 1 and m[j, 0] == 0: #i cooperates j defects, cooperator punishes
                    payoff[i, 0] += w * (-c1 - c2)
                    payoff[j, 0] += w * (b - c3)
              elif m[j, 0] == 1 and m[j, 1] == m[i, 0] == 0: #j cooperates i defects, cooperator does not punish
                    payoff[j, 0] += w * (-c1)
                    payoff[i, 0] += w * b
              elif m[j, 0] == m[j, 1] == 1 and m[i, 0] == 0: #j cooperates i defects, cooperator punishes
                    payoff[j, 0] += w * (-c1 - c2)
                    payoff[i, 0] += w * (b - c3)
              elif m[i, 0] == m[j, 0] == 0 and m[i, 1] == m[j, 1] == 1: #both defect, both punish
                    payoff[i, 0] += w * (-c2 - c3)
                    payoff[j, 0] += w * (-c2 - c3)
              elif m[i, 0] == m[j, 0] == m[i, 1] == m[j, 1] == 0: #both defect, neither punishes
                    payoff[i, 0] += 0
                    payoff[j, 0] += 0
              elif m[i, 0] == m[j, 0] == m[j, 1] == 0 and m[i, 1] == 1: #both defect, i punishes j does not punish
                    payoff[i, 0] += w * (-c2)
                    payoff[j, 0] += w * (-c3)
              elif m[i, 0] == m[j, 0] == m[i, 1] == 0 and m[j, 1] == 1: #both defect, j punishes i does not punish
                    payoff[j, 0] += w * (-c2)
                    payoff[i, 0] += w * (-c3)
          if probs[i, 0] > p and probs[j, 0] < p: #i cannot observe, j observes
              if m[i, 0] == m[i, 1] == 1: #i cooperates and punishes
                    payoff[i, 0] += w * (b - c1)
                    payoff[j, 0] += w * (b - c1)
              elif m[i, 0] == 1 and m[i, 1] == 0: #i cooperates but does not punish
                    payoff[i, 0] += w * (-c1)
                    payoff[j, 0] += w * b
              elif m[i, 0] == 0 and m[i, 1] == m[j, 1] == 1: #i defects and they both punish
                    payoff[i, 0] += w * (b - c3)
                    payoff[j, 0] += w * (-c1 - c2)
              elif m[i, 0] == m[j, 1] == 0 and m[i, 1] == 1: #i defects and only i punishes
                    payoff[i, 0] += w * b
                    payoff[j, 0] += w * (-c1)
              elif m[i, 0] == m[i, 1] == 0 and m[j, 1] == 1: #i defects and only j punishes
                    payoff[i, 0] += w * (-c3)
                    payoff[j, 0] += w * (-c2)
              elif m[i, 0] == m[i, 1] == m[j, 1] == 0: #i defects and neither punishes
                    payoff[i, 0] += 0
                    payoff[j, 0] += 0
          if probs[j, 0] > p and probs[i, 0] < p: #j cannot observe, i observes
              if m[j, 0] == m[j, 1] == 1: #j cooperates and punishes
                    payoff[j, 0] += w * (b - c1)
                    payoff[i, 0] += w * (b - c1)
              elif m[j, 0] == 1 and m[j, 1] == 0: #j cooperates but does not punish
                    payoff[j, 0] += w * (-c1)
                    payoff[i, 0] += w * b
              elif m[j, 0] == 0 and m[j, 1] == m[i, 1] == 1: #j defects and they both punish
                    payoff[j, 0] += w * (b - c3)
                    payoff[i, 0] += w * (-c1 - c2)
              elif m[j, 0] == m[i, 1] == 0 and m[j, 1] == 1: #j defects and only j punishes
                    payoff[j, 0] += w * b
                    payoff[i, 0] += w * (-c1)
              elif m[j, 0] == m[j, 1] == 0 and m[i, 1] == 1: #j defects and only i punishes
                    payoff[j, 0] += w * (-c3)
                    payoff[i, 0] += w * (-c2)
              elif m[j, 0] == m[j, 1] == m[i, 1] == 0: #j defects and neither punishes
                    payoff[j, 0] += 0
                    payoff[i, 0] += 0
          if probs[i, 0] < p and probs[j, 0] < p: #both can observe
                    payoff[i, 0] += w * (m[i, 1] * m[j, 1]) * (b - c1) + w * ((m[i, 1] != m[j, 1])) * (m[i, 1] * b + (1 - m[i, 1]) * (-c1))
                    payoff[j, 0] += w * (m[i, 1] * m[j, 1]) * (b - c1) + w * ((m[i, 1] != m[j, 1])) * (m[j, 1] * b + (1 - m[j, 1]) * (-c1))

    payoff_avg_over_all_interactions = payoff / (2 * (s - 1)) #avg payoffs over all interactions
    normalized_payoff_avg = payoff_avg_over_all_interactions - np.max(payoff_avg_over_all_interactions)
    payoff_proportional_each_player = np.exp(normalized_payoff_avg) / np.sum(np.exp(normalized_payoff_avg)) #proportional payoffs
    #Wright-Fisher process

  #I)MUTATIONS AT REPRODUCTION
    r = np.random.rand(s,1) #random probability vector to determine who reproduces
    mut = np.random.rand(s,2) #if a mutation occurs at location k, first is mutation occurs at cooperation in games where P/NP is not observed, second is mutation occurs at commitment to punishments
    for k in np.arange(s): #determining who is reproducing to the location m[k,:] in the next gen
      v = -1 #indexing the players
      while r[k, 0] > 0: #as long as the selected player at location k is able to reproduce
        v += 1
        r[k, 0] = r[k, 0] - payoff_proportional_each_player[v, 0]
      if mut[k,0] < u: #mutation occurs at cooperation when P/NP is unobserved
        counter1 = counter1 + 1 #counter of mutations occuring at reproduction
        m_new[k,0] = 1-m[v,0]
        if mut[k,1] < u: #mutation occurs at commitment to punishing
          counter1 = counter1 + 1 #counter of mutations occuring at reproduction
          m_new[k,1] = 1-m[v,1]
        else: #no mutation occurs at commitment to punishing
          m_new[k,1] = m[v,1]
      else: #no mutation occurs at cooperation when P/NP is unobserved
        m_new[k,0] = m[v,0]
        if mut[k,1] < u: #mutation occurs at commitment to punishing
          counter1 = counter1 + 1 #counter of mutations occuring
          m_new[k,1] = 1-m[v,1]
        else: #no mutation occurs at commitment to punishing
          m_new[k,1] = m[v,1]

    m = m_new

    avg_cooperation_rate_over_time_not_observed[int(t-1),0] = sum(m[:,0])/s
    avg_punishment_rate_over_time[int(t-1),0] = sum(m[:, 1])/s

    m_total = m_total + m
    t_reached = t

  #II)STOP CHECK FOR REPRODUCTION MUTATIONS

    #this part is adjusted per smaller number of periods
    for j in np.arange(1,5):
      if t == (25000 * j):
        if np.abs(np.sum(avg_cooperation_rate_over_time_not_observed[0:int(t//2)]) - np.sum(avg_cooperation_rate_over_time_not_observed[int(t//2 + 1):int(t-1), 0]))/(int(t)/2) <= epsilon:
          if np.abs(np.sum(avg_punishment_rate_over_time[0:int(t//2)]) - np.sum(avg_punishment_rate_over_time[int(t//2 + 1):int(t - 1), 0]))/(int(t)/2) <= epsilon:
            stop = j
          else:
            pass
        else:
          pass
      else:
        pass

  #III)MUTATIONS DUE TO FIXATION

    if (np.max(m[:, 0]) == np.min(m[:, 0])) and (np.max(m[:, 1]) == np.min(m[:, 1])):
      number_of_reproductions = np.random.geometric(u) #drawing a random nr of reproductions until next mutation
      number_of_time_periods = np.fix(number_of_reproductions/(2*s)) #number of generations passed until next mutation
      mut_location = np.remainder(number_of_reproductions, (2*s)) #mutation location
      if (t + number_of_time_periods) <= t_max: #mutation occurs before the max period is reached
        for d in np.arange(number_of_time_periods):
          avg_cooperation_rate_over_time_not_observed[(int(t_reached) + int(d)), 0] = sum(m[:, 0])/s
          avg_punishment_rate_over_time[(int(t_reached) + int(d)), 0] = sum(m[:, 1])/s

  #IV)STOP CHECK FOR FIXATION MUTATIONS

        #this part is adjusted per smaller number of periods
          for j in np.arange(1,5):
            if t == (25000 * j):
              if np.abs(np.sum(avg_cooperation_rate_over_time_not_observed[0:int((t_reached + int(d)) / 2), 0]) - np.sum(avg_cooperation_rate_over_time_not_observed[int((t_reached + int(d))/ 2 +1): int(t_reached + int(d)), 0]))/(t//2) <= epsilon:
                if np.abs(np.sum(avg_punishment_rate_over_time[0:int((t_reached + int(d)) / 2), 0]) - np.sum(avg_punishment_rate_over_time[int((t_reached + int(d))/ 2 +1): int(t_reached + int(d)), 0]))/(t//2) <= epsilon:
                  stop = j
                else:
                  pass
              else:
                pass
            else:
              pass

        counter2 = counter2 + 1 #counter of how many times mutation occured due to fixation
        if mut_location == 0: #case where there is no mutation anywhere
          m_total = m_total + m * (number_of_time_periods - 1) #periods where nothing happens
          m[s-1, 1] = 1 - m[s-1, 1] #mutant replaces the commitment of the last individual
          t = t + number_of_time_periods #skipping forward the periods where fixation mutation did not happen
          t_reached = t
        elif mut_location > 0: #case where there a mutation somewhere
          m_total = m_total + m * (number_of_time_periods) #periods where nothing happens
          if np.remainder(mut_location, 2) == 1: #mutation is in the cooperation under unobserved cond. strategy
            mut_individual = int(np.fix(mut_location / 2)) + 1
            if mut_individual > s:
              mut_individual = s
            m[mut_individual-1, 0] = 1-m[mut_individual-1, 0] #mutant replaces the cooperation under unobserved cond. strategy at row k
          else: #mutation is in the P/NP commitment strategy
            mut_individual = int(np.fix(mut_location / 2)) + 1
            if mut_individual > s:
              mut_individual = s
            m[mut_individual-1, 1] = 1-m[mut_individual-1, 1] #mutant replaces the P/NP commitment strategy at row k
          mut_index = 0 #now checking for additional mutations within the same generation

  #V)ADDITIONAL FIXATION MUTATIONS WITHIN THE SAME GENERATION

          while mut_index == 0:
            number_of_reproductions2 = np.random.geometric(u) #drawing a random nr of reproductions until next mutation
            if mut_location + number_of_reproductions2 <= (2 * s): #mutation occurs
              counter2 = counter2 + 1 #counter of how many times mutation occured
              mut_location = mut_location + number_of_reproductions2
              if np.remainder(mut_location, 2) == 1: #mutation is in the cooperation under unobserved cond. strategy
                mut_individual = int(np.fix(mut_location / 2)) + 1
                if mut_individual > s:
                  mut_individual = s
                m[mut_individual-1, 0] = 1-m[int(mut_individual)-1, 0] #mutant replaces the cooperation under observed cond. strategy at row k
              else: #mutation is in the P/NP commitment strategy
                mut_individual = int(np.fix(mut_location / 2)) + 1
                if mut_individual > s:
                  mut_individual = s
                m[mut_individual-1, 1] = 1-m[mut_individual-1, 1] #mutant replaces the P/NP commitment strategy at row k
            else: #mutation does not occur
              mut_index = 1

          t = t + number_of_time_periods #going to the new period where mutation occured
          t_reached = t

        m_total = m_total + m #updating population strategies

        avg_cooperation_rate_over_time_not_observed[int(t-1),0] = sum(m[:,0])/s
        avg_punishment_rate_over_time[int(t-1),0] = sum(m[:, 1])/s

      else: #mutation does not occur within the max periods
        for d in np.arange(int(t_max - t_reached)):
          avg_cooperation_rate_over_time_not_observed[(int(t_reached) + int(d)), 0] = sum(m[:, 0])/s
          avg_punishment_rate_over_time[(int(t_reached) + int(d)), 0] = sum(m[:, 1])/s
        m_total = m_total + m * (t_max - t)
        t_reached = t_max
        m_final_cooperation_rate_not_observed = m[:, 0] #unobserved cooperation strategies at the final point, if t_max is reached
        m_final_punishment_rate = m[:, 1] #commitments to punishing at the final point, if t_max is reached
        t = t_max + 1

    t = t + 1

    if stop != 0:
      t = t_max + 1

  m_avg = m_total / t_reached

  summary_cooperation_rate_unobserved = sum(m_avg[:, 0]) / s #avg coop. rate when agents do not observe P/NP commitment
  summary_punishment_rate = sum(m_avg[:, 1]) / s #avg rate of commitment to punishment
  #if summary_punishment_rate > 1:
   # t = t_max +1
    #print("the loop stopped because summary_punishment_rate > 1. The t is", t, "and the p is", p)
    #print("sum(m_avg[:, 1]) is", sum(m_avg[:, 1]), "which is divided by s:", s)
    #print("m_avg is:", m_avg)
    #print("m avg is calculated by dividing  m_total:",  m_total, "to t_reached: ", t_reached)

#filling in the prob. analysis matrices for the graphs
  prob_analysis[int(10*p), 0] = p
  prob_analysis[int(10*p), 1] = summary_cooperation_rate_unobserved
  prob_analysis[int(10*p), 2] = summary_punishment_rate

#filling in the 2 supplemental cooperation & punishment graphs
  if p == 0.0:
    c_snapshots1 = np.concatenate((avg_cooperation_rate_over_time_not_observed[::t_timesteps], np.array([[avg_cooperation_rate_over_time_not_observed[-1, 0]]])))
    p_snapshots1 = np.concatenate((avg_punishment_rate_over_time[::t_timesteps], np.array([[avg_punishment_rate_over_time[-1, 0]]])))

  elif p == 0.5:
    c_snapshots2 = np.concatenate((avg_cooperation_rate_over_time_not_observed[::t_timesteps], np.array([[avg_cooperation_rate_over_time_not_observed[-1, 0]]])))
    p_snapshots2 = np.concatenate((avg_punishment_rate_over_time[::t_timesteps], np.array([[avg_punishment_rate_over_time[-1, 0]]])))
  elif p == 1.0:
    c_snapshots3 = np.concatenate((avg_cooperation_rate_over_time_not_observed[::t_timesteps], np.array([[avg_cooperation_rate_over_time_not_observed[-1, 0]]])))
    p_snapshots3 = np.concatenate((avg_punishment_rate_over_time[::t_timesteps], np.array([[avg_punishment_rate_over_time[-1, 0]]])))
  else:
    pass

#plot for the observability probability analysis
header1 = "version-" + str(version) + "-observability parameter (p) analysis.png"
fig, ax = plt.subplots()
plt.plot(prob_analysis[:, 0], prob_analysis[:, 1], label='Population Cooperation Rate Averages when Unobserved', marker = ".", color ="blue" )
plt.plot(prob_analysis[:, 0], prob_analysis[:, 2], label='Population Punishment Rate Averages', marker = ".", color ="red")
plt.xlabel('p (probability of observing commitments to punishing)')
plt.ylabel('population fraction')
plt.ylim(-0.1, 1.1)
plt.legend()
fig.set_size_inches(12, 8)
plt.savefig(header1)
plt.show()

#plot for the observability (p) parameter analysis
#for commitment strategies
header2 = "version-" + version + "-avg. population % of cooperation over time.png"
fig, ax = plt.subplots()
ax.plot(c_snapshots1, marker = "^", label="p=0.0", color ="red")
ax.plot(c_snapshots2, marker = "o", label="p=0.5", color ="purple")
ax.plot(c_snapshots3, marker = "v", label="p=1.0", color ="blue")
ax.set_xlabel('periods - every 10K')
ax.set_ylabel('avg. population cooperation rate')
ax.set_title('Change in avg. cooperation rate when commitment is unobserved, over time (every 10K periods), at different observability prob. (p) levels ')
plt.ylim(-0.1, 1.1)
ax.legend()
fig.set_size_inches(12, 8)
plt.savefig(header2)
plt.show()

#for punishment strategies
header3 = "version-" + version + "-avg. population % of punishment over time.png"
fig, ax = plt.subplots()
ax.plot(p_snapshots1, marker= "^", label="p=0.0", color ="red")
ax.plot(p_snapshots2, marker= "o",label="p=0.5", color ="purple")
ax.plot(p_snapshots3, marker= "v",label="p=1.0", color ="blue")
ax.set_xlabel('periods - every 10K')
ax.set_ylabel('avg. population punishment rate')
ax.set_title('Change in avg. punishment commitment rate, over time (every 10K periods), at different observability prob. (p) levels ')
plt.ylim(-0.1, 1.1)
ax.legend()
fig.set_size_inches(12, 8)
plt.savefig(header3)
plt.show()

finish_time = datetime.datetime.now()
timestamp_str2 = finish_time.strftime("%Y-%m-%d %H:%M:%S")
print("Starting timestamp:", timestamp_str)
print("Finishing timestamp:", timestamp_str2)

#Simulation summary
header4 = "version-" + str(version) + "-simulation summary.txt"
output_text = "RUNTIME START:" + str(timestamp_str) + "RUNTIME FINISH:" + str(timestamp_str2) + ". Max. number of possible time periods (t_max) is:" + str(t_max) + ". Number of agents (n) is: " + str(s) + ". Intensity of selection (w) is: " + str(w) + ". Mutation prob. (u) is:" + str(u) + ". The stopping rule - epsilon is " + str(epsilon)
output_text += " .Counter1 (how many times mut. happened at reproduction): " + str(counter1) + ", Counter2 (how many times mutation occured due to fixation): " + str(counter2) + ", If the simulation stopped early, the stopped period is: " + str(stop*25000)
with open(header4, 'w') as file:
    file.write(output_text)

#THESE ARE TO STORE OUTPUT DATA FROM THE VM
snapshots_final = np.column_stack((c_snapshots1, c_snapshots2, c_snapshots3, p_snapshots1, p_snapshots2, p_snapshots3))#row1 is coop. rates over time (every 10k gen) when p=0, row2 is when p=0.5 and row3 is when p=1. Row4 is punishment rates over time (every 10k gen) when p=0, row5 is when p=0.5 and row6 is when p=1.
header9 = "version-" + str(version) + "c_and_p_snapshots.csv"
header10 = "version-" + str(version) + "prob_analysis.csv"
np.savetxt(header9, snapshots_final, delimiter=',')
np.savetxt(header10, prob_analysis, delimiter=',')